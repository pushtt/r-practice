---
title: "Model Diagnostics"
author: "Thinh"
date: "7/20/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
```

```{r}
housing <- read_csv("http://www.jaredlander.com/data/housing.csv")

names(housing) <- c("Neighborhood", "Class", "Units", "YearBuilt",  "SqFt", "Income", "IncomePerSqFt", "Expense", "ExpensePerSqFt", "NetIncome", "Value", "ValuePerSqFt", "Boro")
```

Brooklyn and Queens make up one mode and Manhattan makes up the other, while there is not much data on the Bronx and Staten Island

```{r}
housing %>% 
  ggplot(aes(ValuePerSqFt, fill = Boro)) +
  geom_histogram(binwidth = 10) + 
  labs(x = "Value per square foot") + 
  facet_wrap(~ Boro)
```


```{r}
housing %>% 
  ggplot(aes(SqFt)) + 
  geom_histogram()

housing %>% 
  ggplot(aes(Units)) + 
  geom_histogram()

gridExtra::grid.arrange()
```

There are few outliers beyond 1000. It's reasonable to drop them out? Let see some more information 

```{r}
housing %>% 
  ggplot(aes(SqFt, ValuePerSqFt, colour = Units)) + 
  geom_point() + 
  scale_x_continuous(labels = scales::comma) +
  scale_colour_gradient2(low = "green", high = "red", mid = "white", midpoint = 1000)
```


```{r}
housing %>% 
  ggplot(aes(Units, ValuePerSqFt, colour = Units)) + 
  geom_point() + 
  scale_x_continuous(labels = scales::comma) +
  scale_colour_gradient2(low = "green", high = "red", mid = "white", midpoint = 1000)
```

It's reasonable to drop them out. 


# Modelling 

```{r}
house1 <- housing %>% 
  lm(ValuePerSqFt ~ Units + SqFt + Boro, data = .)

summary(house1)

library(broom)
tidy(house1)
```

```{r}
coefplot::coefplot(house1)
```



# How do we judge the quality of a model? 
This could be an analysis of residuals, the results of an ANOVA test or a Wald test, drop-in deviance, the AIC or BIC score, cross-validation error or bootstrapping

## Residuals 

The difference between actual response and the fitted values. 
The idea is if the model is appropriately fitted to the data, the residuals should be **normally distributed**

## Residuals versus Fitted 
```{r}
h1 <- augment(house1) %>% 
  ggplot(aes(.fitted, .resid)) + 
  geom_point() + 
  geom_hline(yintercept = 0, colour = "grey") + 
  geom_smooth(se = FALSE) + 
  labs(
    x = "Fitted Value",
    y = "Residuals",
    title = "Plot of residuals versus fitted values for house1"
  )

h1
```

It's clearly not random.

```{r}
h1 +
  geom_point(aes(color = Boro))
```


## QQ Plot

```{r}
augment(house1) %>% 
  ggplot(aes(sample = .std.resid)) + 
  geom_qq() +
  geom_abline()
```

If the model is a good fit, the standardized residuals should all fall along a straight line when plotted against the theoretical quantiles of the normal distribution.

## Histogram of residuals 

```{r}
augment(house1) %>% 
  ggplot(aes(.resid)) + 
  geom_histogram()
```
This does not look normally distributed, meaning our model is incomplete


## Comparing Models 

```{r}
house2 <- housing %>% 
  lm(ValuePerSqFt ~ Units * SqFt + Boro, data = .)

house3 <- housing %>% 
  lm(ValuePerSqFt ~ Units + SqFt * Boro, data = .)

house4 <- housing %>% 
  lm(ValuePerSqFt ~ Units * SqFt + Boro + SqFt*Class, data = .)

house5 <- housing %>% 
  lm(ValuePerSqFt ~ Boro + Class, data = .)
```


```{r}
coefplot::multiplot(house1, house2, house3, house4, house5)
```

This shows that only Boro and some condominium types matter.
Which model is the best? 

### ANOVA 

```{r}
anova(house1, house2, house3, house4, house5)
```

This shows that the fourth model, house4, has the lowest RSS, meaning it is the best model of the bunch. The problem with RSS is that it always improves when an additional variable is added to the model

### AIC, BIC
Another metric, which penalizes model complexity, is the Akaike Information Criterion (AIC). As with RSS, the model with the lowest AIC—even negative values—is considered optimal. The BIC (Bayesian Information Criterion) is a similar measure where, once again, lower is better.

```{r}
AIC(house1, house2, house3, house4, house5)
```

```{r}
BIC(house1, house2, house3, house4, house5)
```

When called on `glm` models, **anova** returns the deviance of the model, which is another measure of error. The general rule of thumb—according to Andrew Gelman—is that for every added variable in the model, the deviance should drop by two. For categorical (factor) variables, the deviance should drop by two for each level.

```{r}
# create the binary variable based on whether ValuePerSqFt is above 150
housing <- housing %>% 
  mutate(HighValue = ifelse(ValuePerSqFt >= 150, TRUE, FALSE))
```

```{r}
high1 <- housing %>% 
  glm(HighValue ~ Units + SqFt + Boro, 
      data = ., family = binomial(link = "logit"))

high2 <- housing %>% 
  glm(HighValue ~ Units * SqFt + Boro, 
      data = ., family = binomial(link = "logit"))

high3 <- housing %>% 
  glm(HighValue ~ Units + SqFt * Boro + Class, 
      data = ., family = binomial(link = "logit"))

high4 <- housing %>% 
  glm(HighValue ~ Units + SqFt * Boro + SqFt * Class, 
      data = ., family = binomial(link = "logit"))

high5 <- housing %>% 
  glm(HighValue ~ Boro + Class, 
      data = ., family = binomial(link = "logit"))
```


```{r}
anova(high1, high2, high3, high4, high5)
```

```{r}
AIC(high1, high2, high3, high4, high5)

BIC(high1, high2, high3, high4, high5)
```

## Cross Validation

Residual diagnostics and model tests such as ANOVA and AIC are a bit old fashioned and came along before modern computing horsepower.

The preferred method to assess model quality—at least by most data scientists—is cross-validation. The data is broken into k (usually five or ten) non-overlapping sections. Then a model is fitted on k − 1 sections of the data, which is then used to make predictions based on the kth section. This is repeated k times until every section has been held out for testing once and included in model fitting k − 1 times. Cross-validation provides a measure of the predictive accuracy of a model, which is largely considered a good means of assessing model quality.

```{r}
library(boot)

houseG1 <- housing %>% 
  glm(ValuePerSqFt ~ Units + SqFt + Boro, 
      data = ., family = gaussian(link = "identity"))

# ensure it gives the same result as lm

identical(coef(house1), coef(houseG1))

# run cross validation with 5 folds
houseCV1 <- cv.glm(housing, houseG1, K = 5)

# check the error
houseCV1$delta
```

The results from cv.glm include delta, which has two numbers, the raw cross-validation error based on the cost function - mean squared error and the adjusted cross-validation error. This second number compensates for not using leave-one-out cross-validation.

While we got a nice number for the error, it helps us only if we can compare it to other models, so we run the same process for the other models we built, rebuilding them with glm first.


```{r}
houseG2 <- glm(ValuePerSqFt ~ Units * SqFt + Boro, data=housing)
houseG3 <- glm(ValuePerSqFt ~ Units + SqFt * Boro + Class, data=housing)
houseG4 <- glm(ValuePerSqFt ~ Units + SqFt * Boro + SqFt*Class, data=housing)
houseG5 <- glm(ValuePerSqFt ~ Boro + Class, data=housing)
```


```{r}
# run cross validation 
houseCV2 <- cv.glm(housing, houseG2, K=5)
houseCV3 <- cv.glm(housing, houseG3, K=5)
houseCV4 <- cv.glm(housing, houseG4, K=5)
houseCV5 <- cv.glm(housing, houseG5, K=5)
```

```{r}
# check the error results
## build a data.frame of the results
cvResults <- as.data.frame(rbind(houseCV1$delta, houseCV2$delta, houseCV3$delta, houseCV4$delta, houseCV5$delta))

# give better column names
names(cvResults) <- c("Error", "Adjusted.Error")

# add model name
cvResults$Model <- sprintf("houseG%s", 1:5)

# check the results
cvResults
```


```{r}
# check the results
## test with ANOVA 
cvANOVA <- anova(houseG1, houseG2, houseG3, houseG4, houseG5)
cvResults$ANOVA <- cvANOVA$`Resid. Dev`
## test with AIC
cvResults$AIC <- AIC(houseG1, houseG2, houseG3, houseG4, houseG5)$AIC

# reshape for visualisation 

cvResults %>% 
  pivot_longer(-Model, names_to = "Measurement", values_to = "Value") %>% 
  ggplot(aes(Model, Value, group = Measurement, colour = Measurement)) + 
  geom_line() + 
  facet_wrap(~ Measurement, scales = "free_y") + 
  theme(axis.text.x = element_text(angle = 90))
```


# Bootstrap

Measuring uncertainty for confidence intervals.  
The idea is we start with n rows of data. 
* Some statistic is applied to the data. 
* The the data is sampled, creating a new dataset. The new set still has n rows except the are repeats and other rows are entirely missing. 
* The statistic is applied to the new dataset.
* The process is repeated R times (around 1200) which generates an entire distribution for the statistic. This distribution can be used to find the mean and the confidence interval for the statistic. 


```{r}
# import baseball data 
baseball <- plyr::baseball %>% filter(year >= 1990)

baseball %>% 
  summarise(hits = sum(h),
            at_bats = sum(ab),
            hits/at_bats)
```


```{r}
bat.avg <- function(data, indices = 1:NROW(data), hits = "h", at.bats = "ab") {
  sum(data[indices, hits], na.rm = TRUE) /
    sum(data[indices, at.bats], na.rm = TRUE)
}
bat.avg(baseball)
# bootstrap it 
# using the baseball data, call bat.avg  1200 items
# pass indices to the function 

avgBoot <- boot(data = baseball, statistic = bat.avg, R = 1200, stype = "i")

# print original measure and estimates of the biases and standard error 
avgBoot

# print the confidence interval 
boot.ci(avgBoot, conf = .95, type = "norm") 
```

Visualising the distribution is as simple as plotting a histogram 

```{r}
ggplot() +
  geom_histogram(aes(avgBoot$t), fill = "grey", colour = "grey") + 
  geom_vline(xintercept = avgBoot$t0 + c(-1, 1) * 2 * sqrt(var(avgBoot$t)), linetype = 2)
```
Histogram of the batting average bootstrap. The vertical lines are two standard errors from the original estimate in each direction. They make up the bootstrapped 95% confidence interval.


# Stepwise Variable Selection 

A common, though becoming increasingly discouraged, way to select variables for a model is stepwise selection. This is the process of iteratively adding and removing variables from a model and testing the model at each step, usually using AIC

The step function iterates through possible models. The scope argument specifies a lower and upper bound on possible models. The direction argument specifies whether variables are just added into the model, just subtracted from the model or added and subtracted as necessary

```{r}
# the lowest model is the null the model, basically the straight average 
nullModel <- lm(ValuePerSqFt ~ 1, data = housing)
# the largest model we will accept 
fullModel <- lm(ValuePerSqFt ~ Units + SqFt * Boro + Boro * Class, data = housing)
# try different models 
# start with the nullModel 
# do not go above the fullModel 
# work in both directions 
houseStep <- step(nullModel, 
                  scope = list(lower=nullModel, upper = fullModel),
                  direction= "both")


# reveal the chosen model 
houseStep
```
Ultimately, step decided that fullModel was optimal with the lowest AIC. While this works, it is a bit of a brute force method and has its own theoretical problems. Lasso regression arguably does a better job of variable selection

