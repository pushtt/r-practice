---
title: "Regularisation & Shrinkage"
author: "Thinh"
date: "7/25/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
```

In the era of high dimensional (many variables) data, methods are needed to prevent over fitting. Traditionally, it's done with the variable selection such as stepwise forward/backward selection. Modern methods can take a number of forms; we focus on **regularization and shrinkage**

# Elastic Net
A blend of lasso and ridge regression. The lasso use L1 penalty while the ridge uses an L2 penalty. 

`glmnet` which fits generalised linear models with Elastic Net. It requires a matrix of predictors and a response matrix. `model.matrix` is simple and convenient to create a matrix

```{r}
acs <- read.table("http://jaredlander.com/data/acs_ny.csv", sep=",",
                  header = TRUE, stringsAsFactors = FALSE)

testFrame <- as.data.frame(tibble(
  First = sample(1:10, 20, replace = TRUE), 
  Second = sample(1:10, 20, replace = TRUE),
  Third = sample(1:10, 20, replace = TRUE), 
  Fourth = factor(rep(c("Alice", "Bob", "Charlie", "David"), 5)), 
  Fifth = ordered(rep(c("Edward", "Frank", "Georiga", "Hank", "Issac"),4)), 
  Sixth = rep(c("a", "b"), 10)))

head(testFrame)
head(model.matrix(First ~ Second + Fourth + Fifth, testFrame))

```
**Notice:**
* `Fourth` is converted into indicator variables with one less column than `levels` in `fourth`.
* `Fifth`: there is one less than `levels` in `Fifth` and values are not just 1s and 0s. This is because `Fifth` is an `ordered factor`
* Not creating an indicator variable for the base level of a factor is essential for most linear models to avoid multicollinearity.
* However, this is undesirable for Elastic Net. 

```{r}
library(useful)
# always use all levels 
build.x(First  ~ Second + Third + Fourth + Fifth, testFrame, contrast=FALSE)
```

```{r}
# just all levels for Fourth 
build.x(First ~ Second + Third + Fourth + Fifth, testFrame, contrast=c(Fourth=FALSE, Fifth=TRUE))
```

```{r}
# make a binary Income variable for building losgistic regression
acs$Income <- with(acs, FamilyIncome >= 150000)

head(acs)
```

```{r}
# build predictor matrix 
# do not include the intercept as glmnet will add that automatically 

acsX <- build.x(Income ~ NumBedrooms + NumChildren + NumPeople + 
            NumRooms + NumUnits + NumVehicles + NumWorkers +
            OwnRent + YearBuilt + ElectricBill + FoodStamp + 
            HeatingFuel + Insurance + Language - 1, data = acs, contrasts=FALSE)
```

```{r}
class(acsX)

dim(acsX)
```


```{r}
# build response predictor
acsY <- build.y(Income ~ NumBedrooms + NumChildren + NumPeople
                + NumRooms + NumUnits + NumVehicles + NumWorkers 
                + OwnRent + YearBuilt + ElectricBill + FoodStamp
                + HeatingFuel + Insurance + Language - 1, data=acs)
```


```{r}
library(glmnet)
set.seed(1863561)
# run cross-validated glmnet
acsCV1 <- cv.glmnet(x=acsX, y=acsY, family="binomial", nfold=5)
```


```{r}
acsCV1$lambda.mim

acsCV1$lambda.1se
```


```{r}
plot(acsCV1)
```


```{r}
coef(acsCV1, s = "lambda.1se")
```



```{r}
# plot the path 
plot(acsCV1$glmnet.fit, xvar="lambda")
# add vertical lines
abline(v=log(c(acsCV1$lambda.min, acsCV1$lambda.1se)), lty=2)
```


Settiing alpha to 0 causes the results to be the ridge. In this case, every variable is kept in the model but it just shurnk closer to 0. 

```{r}
# fit ridge model 
acsCV2 <- cv.glmnet(x = acsX, y = acsY, family = "binomial", 
                    nfold = 5, alpha = 0)

# look at the lambda value 
acsCV2$lambda.min
acsCV2$lambda.1se

# look at the coefficients
coef(acsCV2, s = "lambda.1se")
```


```{r}
# plot the cross validation error path 
plot(acsCV2)

# plot the coefficient path 
plot(acsCV2$glmnet.fit, xvar = "lambda")
abline(v = log(c(acsCV2$lambda.min, acsCV2$lambda.1se)), lty = 2)
```


# Finding the optimal alpha 
- require an additional layer of cross-validation 

```{r}
library(parallel)
library(doParallel)
```

- It is generally considered better to lean toward the lasso than ridge, so we consider only when alpha > 0.5

```{r}
# reproducibility 
set.seed(2834673)

# create folds
# we want observations to be in the same fold each time it is run
theFolds <- sample(rep(x=1:5, length.out=nrow(acsX)))

# make sequence of alpha values

alphas <- seq(from=.5, to=1, by=.05)
```


```{r}

```

